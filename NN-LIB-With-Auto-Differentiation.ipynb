{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ucu3JOCLDfG3"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fd__gw9CR7YJ"
      },
      "outputs": [],
      "source": [
        "class Tensor:\n",
        "  def __init__(self, value, _parent = None, _op = None, grad = 0, _label = None):\n",
        "    self.value = value\n",
        "    self._parent = _parent\n",
        "    self._op = _op\n",
        "    self.grad = 0\n",
        "    self._label = _label\n",
        "\n",
        "  def __repr__(self):\n",
        "    return f'Label: {self._label}; Value: {self.value}; Grad: {self.grad}'\n",
        "\n",
        "  def backward(self):\n",
        "     pass\n",
        "\n",
        "  def __add__(self, other):\n",
        "    v = self.value + other.value\n",
        "    out = Tensor(v, (self, other), '+')\n",
        "    def backward():\n",
        "\n",
        "      self.grad += out.grad * np.ones_like(self.value)\n",
        "      other.grad += out.grad * np.ones_like(other.value)\n",
        "\n",
        "      self.backward()\n",
        "      other.backward()\n",
        "    out.backward = backward\n",
        "    return out\n",
        "\n",
        "    '''\n",
        "  def dot(self, other):\n",
        "    #1-d dot product, nx1 @ 1xn, seperating bc of the backward fn, self and other for dot product is diff than matmul\n",
        "    v = np.dot(self.value, other.value)\n",
        "    out = Tensor(v, (self, other), '.')\n",
        "    def backward():\n",
        "      self.grad = out.grad * other.value.T\n",
        "      other.grad = out.grad * self.value.T\n",
        "\n",
        "      self.backward()\n",
        "      other.backward()\n",
        "    out.backward = backward\n",
        "    return out\n",
        "    '''\n",
        "\n",
        "  def matmul(self, other):\n",
        "    #self = nxd, other = dx1, i.e. self is the weight matrix and other is the input\n",
        "    v = np.matmul(self.value, other.value)\n",
        "    out = Tensor(v, (self, other), '@')\n",
        "    def backward():\n",
        "      #need to see fix the diemnsioning, see if any of the dimensions = 1\n",
        "      #TODO: - After fixing dimensioning, we need to doubling checking the gradient calculations, then it should be fine honestly. After that we need to making the neural net class, we need call backward on child, then all parents.\n",
        "      if (1 not in self.value.shape) and (1 not in other.value.shape):\n",
        "          print(self.value.shape)\n",
        "          # Both operands are matrices\n",
        "          self.grad += out.grad @ other.value.T\n",
        "          other.grad += self.value.T @ out.grad\n",
        "      elif (1 not in self.value.shape) and (1 in other.value.shape):\n",
        "          # self is a matrix, other is a column vector\n",
        "          self.grad += out.grad @ other.value.T\n",
        "          other.grad += (self.value.T) @ (out.grad)\n",
        "\n",
        "      elif (1 in self.value.shape) and (1 not in other.value.shape):\n",
        "          # self is a row vector, other is a matrix\n",
        "          self.grad += out.grad @ other.value.T\n",
        "          other.grad += self.value @ out.grad\n",
        "      else:\n",
        "          # Both operands are vectors, handle as dot product\n",
        "          #self is 1xn, other is nx1\n",
        "          assert self.value.shape[1] == other.value.shape[0]\n",
        "          self.grad += other.value.T * out.grad\n",
        "          other.grad += self.value.T * out.grad\n",
        "      self.backward()\n",
        "      other.backward()\n",
        "    out.backward = backward\n",
        "    return out\n",
        "\n",
        "  def relu(self):\n",
        "    v = np.maximum(0, self.value)\n",
        "    out = Tensor(v, (self), 'ReLU')\n",
        "\n",
        "    def backward():\n",
        "      self.grad = np.where(self.value > 0, 1, 0)\n",
        "      self.backward()\n",
        "    out.backward = backward\n",
        "    return out\n",
        "\n",
        "  def mse(self, y):\n",
        "     v = np.mean(np.power(self.value - y, 2))\n",
        "     out = Tensor(v, (self), 'MSE')\n",
        "\n",
        "     def backward():\n",
        "        self.grad = 2 * (self.value - y) / np.size(y)\n",
        "        self.backward()\n",
        "     out.backward = backward\n",
        "     return out\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lfvB_Mou0MlO"
      },
      "outputs": [],
      "source": [
        "class Dense_Layer:\n",
        "  def __init__(self, input_size, output_size):\n",
        "    self.weights = Tensor(np.random.normal(size=(output_size, input_size)).astype(np.float64))\n",
        "    self.bias = Tensor(np.zeros((output_size, 1)).astype(np.float64))\n",
        "\n",
        "  def forward(self, input):\n",
        "    self.input = input\n",
        "    z_pre_bias = self.weights.matmul(input)\n",
        "    z = z_pre_bias + self.bias\n",
        "\n",
        "    #setting grads to 0, since grads from previosu forward pass need sot be cleared\n",
        "    input.grad = np.zeros_like(input.value)\n",
        "    self.weights.grad = np.zeros_like(self.weights.value)\n",
        "    self.bias.grad = np.zeros_like(self.bias.value)\n",
        "    z_pre_bias.grad = np.zeros_like(z_pre_bias.value)\n",
        "    z.grad = np.zeros_like(z.value)\n",
        "    return z\n",
        "\n",
        "  def backward(self, learning_rate):\n",
        "    #zero the gradients\n",
        "    #add minus operation\n",
        "    self.weights.value -= learning_rate * self.weights.grad\n",
        "    self.bias.value -= learning_rate * self.bias.grad\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bo-IK0ljDfG5"
      },
      "outputs": [],
      "source": [
        "class ReLU_Layer:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "    def forward(self, input):\n",
        "        self.input = input\n",
        "        output = input.relu()\n",
        "\n",
        "        #need to zero grads from previous forward pass\n",
        "        input.grad = np.zeros_like(input.value)\n",
        "        output.grad = np.zeros_like(output.value)\n",
        "        return output\n",
        "    def backward(self, learning_rate):\n",
        "        pass\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OqGY5Q8yDfG6"
      },
      "outputs": [],
      "source": [
        "class MSE_Layer:\n",
        "    def __init__(self):\n",
        "        pass\n",
        "    def forward(self, input, y):\n",
        "        self.input = input\n",
        "        output = input.mse(y)\n",
        "\n",
        "        #need to zero grads from previous forward pass\n",
        "        input.grad = np.zeros_like(input.value)\n",
        "        output.grad = np.zeros_like(output.value)\n",
        "        return output\n",
        "    def backward(self, learning_rate):\n",
        "        pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eYbW-mykDfG7"
      },
      "outputs": [],
      "source": [
        "class MLP:\n",
        "    def __init__(self, layers):\n",
        "        self.layers = layers\n",
        "    def forward(self, input, y):\n",
        "        input_l = input\n",
        "        for layer in self.layers[:-1]:\n",
        "            #print(\"inputs: \")\n",
        "            #print(input_l)\n",
        "            input_l = layer.forward(input_l)\n",
        "        self.output = input_l\n",
        "        self.loss = self.layers[-1].forward(input_l, y)\n",
        "        return input_l\n",
        "    def backward(self, learning_rate):\n",
        "        self.loss.grad = np.ones_like(self.output.value)\n",
        "        self.loss.backward()\n",
        "\n",
        "        for layer in reversed(self.layers):\n",
        "            layer.backward(learning_rate)\n",
        "        #print('GRADS')\n",
        "        #print(self.loss._parent)\n",
        "        #print(self.loss._parent._parent)\n",
        "\n",
        "    def train(self, epochs, learning_rate, data_loader):\n",
        "        for _ in range(0, epochs):\n",
        "            error = 0\n",
        "            for input_data, desired_output in data_loader:\n",
        "                prediction = self.forward(input_data, desired_output)\n",
        "                error = self.loss\n",
        "                self.backward(learning_rate)\n",
        "            #error /= data_loader.batch_size\n",
        "            print(f\"Error for epoch {_}: {error} \")\n",
        "            print(f'input: {input_data}')\n",
        "            print(f'prediction: {self.output}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GI6etBZyDfG7"
      },
      "outputs": [],
      "source": [
        "layer_list = [\n",
        "    Dense_Layer(2, 3),\n",
        "    ReLU_Layer(),\n",
        "    Dense_Layer(3, 1),\n",
        "    ReLU_Layer(),\n",
        "    MSE_Layer()\n",
        "]\n",
        "\n",
        "network = MLP(layer_list)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1ryDe48pDfG8"
      },
      "outputs": [],
      "source": [
        "#testing XOR\n",
        "network.train(100, 0.1, dataloader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D22krxe88txJ"
      },
      "outputs": [],
      "source": [
        "\n",
        "class DataLoader:\n",
        "    def __init__(self, inputs, desired_outputs, batch_size, shuffle=True):\n",
        "        self.inputs = inputs\n",
        "        self.desired_outputs = desired_outputs\n",
        "        self.batch_size = batch_size\n",
        "        self.shuffle = shuffle\n",
        "\n",
        "    def __iter__(self):\n",
        "        # Get the total number of data points\n",
        "        self.n_samples = self.inputs.shape[0]\n",
        "\n",
        "        # Create an array of indices\n",
        "        self.indices = np.arange(self.n_samples)\n",
        "\n",
        "        # Shuffle if required\n",
        "        if self.shuffle:\n",
        "            np.random.shuffle(self.indices)\n",
        "\n",
        "        return self\n",
        "\n",
        "    def __next__(self):\n",
        "        # If all data has been seen, stop the iteration\n",
        "        if len(self.indices) == 0:\n",
        "            raise StopIteration\n",
        "\n",
        "        # Select indices for the current batch\n",
        "        current_indices = self.indices[:self.batch_size]\n",
        "        self.indices = self.indices[self.batch_size:]\n",
        "\n",
        "        # Extract the batch of data\n",
        "        batch_inputs = Tensor(self.inputs[current_indices].T)\n",
        "        batch_outputs = self.desired_outputs[current_indices]\n",
        "\n",
        "        return batch_inputs, batch_outputs\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EhF2vDRP89KM",
        "outputId": "d4520a0a-416c-4267-fdf7-e1b8aecb8c93"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0. 0.]\n",
            " [0. 1.]\n",
            " [1. 0.]\n",
            " [1. 1.]]\n",
            "Batch inputs: Label: None; Value: [[0.]\n",
            " [0.]]; Grad: 0\n",
            "Batch outputs: [[0.]]\n",
            "Batch inputs: Label: None; Value: [[1.]\n",
            " [0.]]; Grad: 0\n",
            "Batch outputs: [[1.]]\n",
            "Batch inputs: Label: None; Value: [[0.]\n",
            " [1.]]; Grad: 0\n",
            "Batch outputs: [[1.]]\n",
            "Batch inputs: Label: None; Value: [[1.]\n",
            " [1.]]; Grad: 0\n",
            "Batch outputs: [[0.]]\n"
          ]
        }
      ],
      "source": [
        "# Example usage\n",
        "inputs = np.reshape(([0, 0], [0, 1], [1, 0], [1 , 1]), (4, 2)).astype(np.float64) # 100 samples, 10 features each\n",
        "#inputs = inputs.T\n",
        "print(inputs)\n",
        "#inputs = inputs.T\n",
        "desired_outputs = np.reshape((0, 1, 1, 0), (4, 1)).astype(np.float64)  # 100 samples, 1 output each\n",
        "batch_size = 1\n",
        "\n",
        "dataloader = DataLoader(inputs, desired_outputs, batch_size)\n",
        "\n",
        "\n",
        "for batch_inputs, batch_outputs in dataloader:\n",
        "    print(\"Batch inputs:\", batch_inputs)\n",
        "    print(\"Batch outputs:\", batch_outputs)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
